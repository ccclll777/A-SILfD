env:
  env_name: Ant-v2
  demonstrate_path: demonstrates/opolo_expert/ant-v2.pkl
  mix_demonstrate_path: demonstrates/mix/ant-mix.pkl
  sub_demonstrate_path: demonstrates/sail_sub_optimal_expert/ant-1200.pkl
  base_reward: 1
  r_max: 6.918 #hooper每个step的最大reward
  max_reward_scale: 2
misc: # Project sharing parameters
  episodes: 5000 # maximum iteration 2000 episode
  episode_max_steps: 1000 # each episode  1000 setps
  num_steps: 1100000
  # maximum iteration 2048000 steps
  evaluate_episode: 10
  evaluate_freq: 3
  evaluate_freq_steps: 2000
  ppo_evaluate_freq_steps: 4096
ours :
  teacher_buffer_size: 10000
  expert_ratio: 0.25 #训练时中专家轨迹的比例
  bc_pre_train: false
  bc_model_path: bc/ant.pth
  sub_bc_model_path: bc/ant_sub.pth
  lamba: 1.0 #策略约束权重
  lamba_min: 0.1 #策略约束权重 最小值
  lamba_decay: true
  lamba_decay_rate: 0.02
  lamba_decay_start: 0
  lamba_decay_freq: 1000
  policy_loss_mode: bc_loss #策略loss的模型 basic/bc_loss/expert_constraint
redq :
  critic_num: 10
  pretrain_demo: false
  pretrain_epoch: 10000 #50000steps有效果但是一般150000step太多了
td3:
  hidden_width: 256
  actor_lr: 0.001
  critic_lr: 0.001
  tau: 0.005
  gamma: 0.99
  policy_noise: 0.2
  expl_noise: 0.1
  noise_clip: 0.5
  target_update_interval: 2
  buffer_size: 1000000
  priority_buffer: false
  batch_size: 256
  start_steps: 10000 #开始采用随即动作初始化的步骤数
